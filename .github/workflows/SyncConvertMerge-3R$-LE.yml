name: SyncConvertMerge 3R $ LE

on: 
  workflow_dispatch: # allows manual workflow run from the Actions tab (workflow must be in default branch to work)
    inputs: # optional input(s) that the cronjob service can pass on to the workflow
      message:
        description: 'source trigger of the workflow run'
        required: true
        default: 'manual GitHub WebUI trigger' # default message to be printed if workflow is triggered manually via WebUI or if there is no input message alongside a trigger by cronjob service
        type: string # optionally, specifies type of message
  # internal scehduled run parameters in the 2 lines below 'commented out' as external cronjob scheduler service is engaged
  # schedule: # scheduled to run at designated times or every x minutes (minimum interval allowed is 5 minutes)
    # - cron: '*/22 * * * *'  
      
env:
  CRONJOB_SERVICE: "FastCron"
  TZ: 'Asia/Kolkata'

concurrency: # ensures that only one GitHub workflow runs at a time for a specific branch in a repo, prevents conditions where two separate workflow runs attempt to push to same branch simultaneously
  group: sync-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: false # 'false' ensures that a newly-started workflow runs sequentially after and does not terminate a still in-progress workflow run, ensuring that every workflow's changes are eventually pushed

jobs:   
  download_files: # copies specified playlists from source repos and overwrites corresponding specified playlists in destination branch/directories in destination repo
    name: Download M3U Playlists & JSONs
    runs-on: ubuntu-latest
    continue-on-error: true # ensures job continues even if it faces issues
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:
      - name: Validate and Load JSON Config from Repo Secrets
        shell: bash
        env:
          CONFIG_JSON: ${{ secrets.ENV_DOWNLOAD_LE }}
        run: |
          # Step 1: Validate JSON syntax
          if ! echo "$CONFIG_JSON" | jq empty; then
            echo "ERROR: INVALID JSON detected in repo secret! Check syntax for missing brackets or trailing commas."
            exit 1
          fi          
          
          # Step 2: Export JSON keys/values to GitHub environment
          echo "$CONFIG_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' >> $GITHUB_ENV # extracts all keys and values and writes them to the global $GITHUB_ENV
          
          # Step 3: Mask individual variables for public log security
          echo "$CONFIG_JSON" | jq -r '.[]' | while read -r value; do
            [ -n "$value" ] && echo "::add-mask::$value" # ensures that even after parsing, individual parsed variables are masked as '***' in logs
          done
          echo "SUCCESS: JSON configuration validated and loaded into GitHub environment."
      
      - name: Checkout Destination Branch in Download Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DOWNLOAD_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DOWNLOAD_BRANCH }}  # set the destination branch to checkout
          token: ${{ secrets.PAT_FG }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'download-branch'

      - name: Download Playlists from Source URLs 
        run: |
          # Step 1: Define the playlist array
          LIST=$(cat <<EOF # defines the list of playlists and jsons to process in a pipe-separated array (URL|DIR|FILE)
          ${{ env.SOURCE_slle1_URL }}|sl-le/m3u|sl-le-1.m3u
          ${{ env.SOURCE_slle2_URL }}|sl-le/m3u|sl-le-2.m3u
          ${{ env.SOURCE_slle3_URL }}|sl-le/m3u|sl-le-3.m3u
          ${{ env.SOURCE_slle4_URL }}|sl-le/json|sl-le-4.json
          ${{ env.SOURCE_fcle1_URL }}|fc-le/m3u|fc-le-1.m3u
          ${{ env.SOURCE_fcle2_URL }}|fc-le/m3u|fc-le-2.m3u
          ${{ env.SOURCE_fcle3_URL }}|fc-le/m3u|fc-le-3.m3u
          ${{ env.SOURCE_fcle4_URL }}|fc-le/m3u|fc-le-4.m3u
          ${{ env.SOURCE_fcle5_URL }}|fc-le/json|fc-le-5.json
          EOF
          )

          # Step 2: Initialize the metadata file 
          METADATA_FILE="${{ github.workspace }}/remote_timestamps.meta" # initializes the metadata file file to record source playlists' latest remote update timestamps
          touch "$METADATA_FILE" # ensures the initialized file exists

          # Step 3: Download playlists and sync remote timestamps
          echo "$LIST" | while IFS="|" read -r URL DIR FILE; do # extracts data from the playlist array string using IFS pipe separator
            [ -z "$URL" ] && continue
            TARGET="download-branch/$DIR/$FILE" # sets destination file path for each playlist download
            mkdir -p "download-branch/$DIR" # creates destination directory if it doesn't already exist

            # Sub-step 3a: Download each playlist from source repos
            CURL_FLAGS="--connect-timeout 3 --retry 3 --retry-delay 1 --retry-max-time 10 --retry-all-errors --no-progress-meter" # optional curl flags
            HTTP_CODE=$(curl $CURL_FLAGS -R -L --fail \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              -H "User-Agent: FileSync-Batch-2026" \
              -o "$TARGET" -w "%{http_code}" "$URL" || echo "000") # downloads the playlist using curl and captures the HTTP status code to verify download success

            # Sub-step 3b: Fetch and sync remote commit timestamp, and echo success/failure with key metrics for each download
            if [ "$HTTP_CODE" = "200" ] && [ -f "$TARGET" ]; then # checks if playlist exists AND was actually downloaded during this specific workflow run
         
              # module: fetch remote commit timestamps
              REMOTE_DATE="" # starts afresh for this iteration
              TIME_NOTE="(standard download)" # default value
              if [[ "$URL" == *"github.com"* ]] || [[ "$URL" == *"githubusercontent.com"* ]]; then
            
                # sub-module: for downloads from GitHub URLs, capture remote commit timestamps from GitHub API 
                OWNER_REPO=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/([^/]+/[^/]+).*|\2|') # extracts 'owner/repo' to call the GitHub API
                FILE_PATH=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/[^/]+/[^/]+/(blob/\|raw/)?([^/]+/)?(.*)|\4|') # extracts 'path/to/file' to call the GitHub API
                API_URL="https://api.github.com/repos/${OWNER_REPO}/commits?path=${FILE_PATH}&per_page=1" # sets the URL to target the playlist's commit history
                REMOTE_DATE=$(curl -s \
                  -H "Authorization: Bearer ${{ secrets.PAT_FG }}" \
                  -H "X-GitHub-Api-Version: 2022-11-28" \
                  -H "User-Agent: GitHub-Actions-FileSync" \
                  "$API_URL" | jq -r '.[0].commit.committer.date' 2>/dev/null || echo "") # captures the remote commit timestamp by querying GitHub API
                [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ] && TIME_NOTE="([✓]API verified)" # updates the timestamp tag
              else
                # sub-module: for downloads from non-GitHub URLs, fallback to capturing remote commit timestamps from HTTP header   
                HEADER_VAL=$(curl -sI "$URL" | grep -i 'Last-Modified:' | cut -d':' -f2- | xargs) # captures the remote commit timestamp from the remote file's header metadata
                if [ -n "$HEADER_VAL" ]; then
                  REMOTE_DATE=$(date -d "$HEADER_VAL" -u +"%Y-%m-%dT%H:%M:%SZ" 2>/dev/null)
                  TIME_NOTE="([✓]header verified)" # updates the timestamp tag
                fi
              fi
              
              # module: sync remote commit timestamps
              if [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ]; then
                touch -d "$REMOTE_DATE" "$TARGET" # manually syncs the downloaded file's timestamp to match remote commit date
                MOD_TIME=$(date -d "$REMOTE_DATE" "+%d-%m-%Y %H:%M IST") # formats and converts timestamp to IST
              else
                MOD_TIME=$(date -r "$TARGET" "+%d-%m-%Y %H:%M IST") # fallback to downloaded playlist's local commit (workflow run) timestamp in case fetching of remote timestamp via GitHub API or HTTP header fails
                TIME_NOTE="([x]m3u commit time)" # updates the timestamp tag
              fi
              
              # module: echo success/failure of each downloaded playlist with key metrics
              SIZE=$(stat -c%s "$TARGET") # calculates file size of downloaded playlist
              COUNT=$(grep -c '#EXTINF' "$TARGET" || true) # counts number of #EXTINF lines in downloaded playlist
              echo "$FILE download : SUCCESS. Last updated $MOD_TIME $TIME_NOTE. $SIZE bytes, $COUNT #EXTINF lines."
              echo "$DIR|$FILE|$SIZE|$COUNT|$MOD_TIME|$TIME_NOTE" >> remote_timestamps.meta # record each playlist's result to metadata fragment (format: DIR|FILE|SIZE|COUNT|TIMESTAMP|TIMENOTE)   
            else
              echo "$FILE download : FAILED (HTTP: $HTTP_CODE)"
              echo "$DIR|$FILE|FAILED|0|N/A|N/A" >> remote_timestamps.meta
            fi
            unset MOD_TIME # clears for next iteration
          done

          # Step 4: Clean up temporary work files       
          rm -f *.tmp *.temp temp_* # 

      - name: Echo Group-Level Download Results
        run: |
          # Step 1: Echo group-level download results
          DIRS=$(cut -d'|' -f1 remote_timestamps.meta | sort -u) # extract unique directories from the consolidated metadata file
          echo "------------------------------------------------------------------------------------------------------------------------"
          for DIR in $DIRS; do
            S_COUNT=0; T_EXTINF=0; T_SIZE=0; F_COUNT=0; F_NAMES=""
            
            # Sub-step 1a: Aggregate group-level download stats for this specific folder
            while IFS="|" read -r f_dir f_name f_size f_extinf f_time f_note; do
              if [ "$f_size" = "FAILED" ]; then
                F_COUNT=$((F_COUNT + 1)) # count of playlists for which download failed 
                F_NAMES+="$f_name " # names of playlists for which download failed 
              else
                S_COUNT=$((S_COUNT + 1)) # count of playlists for which download succeeded
                T_EXTINF=$((T_EXTINF + f_extinf)) # count of #EXTINF lines in playlists for which download succeeded
                T_SIZE=$((T_SIZE + f_size)) # total size of playlists for which download succeeded
              fi
            done < <(grep "^$DIR|" remote_timestamps.meta)
            
            # Sub-step 1b: Echo group-level download results with key metrics
            echo "RESULT for folder $DIR : successfully downloaded $S_COUNT files ($T_SIZE bytes). Total $T_EXTINF #EXTINF lines present."
            echo "RESULT for folder $DIR : failed to download $F_COUNT files: ${F_NAMES:-(none)}"
            echo "------------------------------------------------------------------------------------------------------------------------"
          done

      - name: Commit Changes and Push to Destination Branch
        run: |
          # Step 1: Enter destination directory
          cd download-branch # changes current working directory to the destination branch    

          # Step 2: Configure git          
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG }}@github.com/${{ env.DOWNLOAD_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
          
          # Step 3: Analyse downloaded playlists for final summary echos, and stage changed/new files for commit 
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""
          while IFS="|" read -r DIR FILE SIZE COUNT TIMESTAMP NOTE; do
            TARGET="$DIR/$FILE"
            if [ -f "$TARGET" ]; then
              if [[ -n $(git status --porcelain "$TARGET") ]]; then # uses git status to determine the file state
                COMMITTED_NAMES+="$FILE " # playlist is changed or new and will be committed; adds to committed list
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SUMMARY_DETAILS+="\n  - $FILE : last updated $TIMESTAMP $NOTE - $SIZE bytes, $COUNT #EXTINF lines"
                git add "$TARGET" # stages only the changed/new files selectively based on the git status check 
              else
                SKIPPED_NAMES+="$FILE " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for playlist $FILE."
              fi
            fi
          done < ../remote_timestamps.meta

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit in any of the $SKIPPED_COUNT playlists. Skipping push."
          else
            git commit -m "Automated sync from source" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DOWNLOAD_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DOWNLOAD_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Successfully committed and pushed $COMMITTED_COUNT updated playlist(s)."
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT playlist(s) were found unchanged and were skipped: ${SKIPPED_NAMES:-(none)}"
          echo "------------------------------------------------------------------------------------------------------------------------"

      - name: Upload Artifact with Remote Timestamps Metadata
        if: always() # ensures metadata is uploaded even if some downloads failed
        uses: actions/upload-artifact@v4
        with:
          name: playlist-metadata
          path: ${{ github.workspace }}/remote_timestamps.meta # source-of-truth for all remote commit timestamps
          if-no-files-found: error # forces this job to fail if the playlist-metadata file is missing



  convert_json:
    name: Generate M3U Playlists from JSONs
    needs: download_files # dependency - this job starts only after the referenced job completes/commits
    if: always() # this job runs regardless of other jobs' success/failure status
    runs-on: ubuntu-latest
    continue-on-error: true # prevents one conversion error from failing the whole workflow
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:
      - name: Validate and Load JSON Config from Repo Secrets
        shell: bash
        env:
          CONFIG_JSON: ${{ secrets.ENV_CONVERT_LE }}
        run: |
          # Step 1: Validate JSON syntax
          if ! echo "$CONFIG_JSON" | jq empty; then
            echo "ERROR: INVALID JSON detected in repo secret! Check syntax for missing brackets or trailing commas."
            exit 1
          fi          
          
          # Step 2: Export JSON keys/values to GitHub environment
          echo "$CONFIG_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' >> $GITHUB_ENV # extracts all keys and values and writes them to the global $GITHUB_ENV
          
          # Step 3: Mask individual variables for public log security
          echo "$CONFIG_JSON" | jq -r '.[]' | while read -r value; do
            [ -n "$value" ] && echo "::add-mask::$value" # ensures that even after parsing, individual parsed variables are masked as '***' in logs
          done
          echo "SUCCESS: JSON configuration validated and loaded into GitHub environment."

      - name: Checkout Destination Branch in Download Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DOWNLOAD_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DOWNLOAD_BRANCH }}  # set the destination branch to checkout
          token: ${{ secrets.PAT_FG }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'download-branch'

      - name: Download Artifact with Remote Timestamps Metadata
        uses: actions/download-artifact@v4
        with:
          name: playlist-metadata 
          path: ${{ github.workspace }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Convert JSON to M3U Playlists
        shell: bash
        run: |
          META_FILE="${{ github.workspace }}/remote_timestamps.meta" # defines absolute path for the meta file to ensure updates persist
          cd download-branch # changes current working directory to the destination branch 
          
          # Step 1: Identify active groups from job 1 metadata
          ACTIVE_GROUPS=$(cut -d'|' -f1 "$META_FILE" 2>/dev/null | sed 's|\(.*\)/.*|\1|' | sort -u || echo "") # gets parent folder name by stripping the /json or /m3u sub-folder suffix from meta file entries

          # Step 2: Loop through each active parent folder
          for GROUP in $ACTIVE_GROUPS; do # active groups are all those parent folders that have m3u and/or json subfolder touched by this workflow in job 1 
            JSON_DIR="$GROUP/json"

            # Sub-step 2a: Skip each active parent folder without a json subfolder
            [ ! -d "$JSON_DIR" ] && continue # skips this specific folder (active group) if it doesn't have a 'json' subfolder, processes others         

            # Sub-step 2b: Find all JSON files in the current group and map each JSON file to its specific python conversion script
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "PROCESSING JSONs in group $GROUP..."
            find "$JSON_DIR" -name "*.json" | while read -r JSON_PATH; do # finds all JSON files (pre-existing and new) in the current group
              JSON_FILE=$(basename "$JSON_PATH")
              FILE_ID="${JSON_FILE%.json}"
              M3U_FILE="${FILE_ID}.m3u" # explicitly define for echo statements
              SPECIFIC_SCRIPT="scripts/${FILE_ID}.py" # maps JSON file to its specific unique python script stored in the 'scripts' folder at the root directory of 'download-branch' (e.g., scripts/sl-le-4.py)
              M3U_DIR="$GROUP/m3u" # sets target m3u folder 
              TARGET_M3U="$M3U_DIR/${FILE_ID}.m3u" # sets target m3u file path
              mkdir -p "$M3U_DIR" # ensures target m3u folder exists

              # module: execute python conversion with argument passthrough
              if [ -f "$SPECIFIC_SCRIPT" ]; then # checks if the required python script file is present
                if python3 "$SPECIFIC_SCRIPT" "$JSON_PATH" "$TARGET_M3U"; then
                  
                  # sub-module: sync timestamp onto the generated m3u playlist
                  ORIG_IST=$(grep "|$JSON_FILE|" "$META_FILE" | cut -d'|' -f5 || echo "") # extracts the json's origonal remote timestamp from the metadata file
                  if [ -f "$TARGET_M3U" ]; then

                    # sub-module: if remote timestamp of json exists (json was created/updated during this workflow run), sync the original remote timestamp onto the m3u 
                    if [ -n "$ORIG_IST" ]; then
                      CLEAN_DATE=$(echo "$ORIG_IST" | awk '{print $1}' | awk -F'-' '{print $3"-"$2"-"$1}') # converts <DD-MM-YYYY HH:MM IST> back to ISO for the 'touch' command
                      CLEAN_TIME=$(echo "$ORIG_IST" | awk '{print $2}')
                      touch -d "$CLEAN_DATE $CLEAN_TIME" "$TARGET_M3U" || true # syncs the timestamp
                      MOD_TIME="$ORIG_IST"
                      TIME_NOTE="([✓]API verified)"
                    # sub-module: if remote timestamp of json does not exist (json was pre-existing), fallback to syncing JSON's local modification time onto the m3u
                    else
                      touch -r "$JSON_PATH" "$TARGET_M3U" || true
                      MOD_TIME=$(date -r "$TARGET_M3U" "+%d-%m-%Y %H:%M IST")
                      TIME_NOTE="([x]json commit time)"
                    fi
                    
                    # sub-module: success echo with metrics
                    SIZE=$(stat -c%s "$TARGET_M3U")
                    COUNT=$(grep -c '#EXTINF' "$TARGET_M3U" || echo "0")
                    echo "$M3U_FILE conversion : SUCCESS. Last updated $MOD_TIME $TIME_NOTE. $SIZE bytes, $COUNT #EXTINF lines."

                    # sub-module: update the metadata file with the new M3U info
                    sed -i "\:^$M3U_DIR|$M3U_FILE:d" "$META_FILE" # first remove any old entry in the metadata file for this specific M3U, then append new one
                    echo "$M3U_DIR|$M3U_FILE|$SIZE|$COUNT|$MOD_TIME|$TIME_NOTE" >> "$META_FILE"  
                  
                  else
                    # sub-module: failure echo if python script finished but output m3u playlist is missing
                    echo "$JSON_FILE conversion : FAILED (output file missing)."
                  fi
                else
                  # sub-module: failure echo if python script itself crashed
                  echo "$JSON_FILE conversion : FAILED (python script error)."
                fi
              else
                # module: failure echo if no matching python script was found for the JSON
                echo "$JSON_FILE conversion : SKIPPED (no python script found at $SPECIFIC_SCRIPT)."
              fi
            done
          done
          echo "------------------------------------------------------------------------------------------------------------------------"

      - name: Commit and Push Generated Playlists
        run: |
          # Step 1: Enter destination directory
          cd download-branch # changes current working directory to the destination branch 
          
          # Step 2: Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG }}@github.com/${{ env.DOWNLOAD_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
          
          # Step 3: Stage newly generated M3U playlists
          git add . 
          
          # Step 3: Conditional Push
          if git diff --cached --quiet; then # checks if there are any changes
            echo "RESULT: No changes found to commit in any generated M3U playlists. Skipping push."
          else
            git commit -m "Automated generation from JSON" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DOWNLOAD_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DOWNLOAD_BRANCH }} || echo "FAILURE: Git push failed. Skipping." # pushes to the destination branch
            echo "RESULT: Successfully committed and pushed generated M3U playlists."
          fi

      - name: Re-upload Artifact with Remote Timestamps Metadata
        if: always() # ensures metadata is uploaded even if some conversions failed
        uses: actions/upload-artifact@v4
        with:
          name: final-playlist-metadata # has merged metadata of job 1 m3u downloads and job 2 json conversions
          path: ${{ github.workspace }}/remote_timestamps.meta # final source-of-truth for all remote commit timestamps
          if-no-files-found: error # forces this job to fail if the playlist-metadata file is missing



  merge_playlists: # creates custom-formatted deduplicated merged playlists with prioritized m3u comtent-blocks from input playlists
    name: Create Merged Playlists
    needs: [download_files, convert_json] # forces this job to wait for the referenced job(s) to finish (forces sequential instead of the default parallel execution)
    if: always() # this job runs regardless of other jobs' success/failure status
    runs-on: ubuntu-latest
    continue-on-error: true # prevents one conversion error from failing the whole workflow
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:          
      - name: Validate and Load JSON Config from Repo Secrets
        shell: bash
        env:
          CONFIG_JSON: ${{ secrets.ENV_MERGE_LE }}
        run: |
          # Step 1: Validate JSON syntax
          if ! echo "$CONFIG_JSON" | jq empty; then
            echo "ERROR: INVALID JSON detected in repo secret! Check syntax for missing brackets or trailing commas."
            exit 1
          fi          
          
          # Step 2: Export JSON keys/values to GitHub environment
          echo "$CONFIG_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' >> $GITHUB_ENV # extracts all keys and values and writes them to the global $GITHUB_ENV
          
          # Step 3: Mask individual variables for public log security
          echo "$CONFIG_JSON" | jq -r '.[]' | while read -r value; do
            [ -n "$value" ] && echo "::add-mask::$value" # ensures that even after parsing, individual parsed variables are masked as '***' in logs
          done
          echo "SUCCESS: JSON configuration validated and loaded into GitHub environment."

      - name: Checkout Source Branch in Download Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DOWNLOAD_REPO }}' # specify 'owner/repo' of the source repo
          ref: ${{ env.DOWNLOAD_BRANCH }}  # set the source branch to checkout
          token: ${{ secrets.PAT_FG }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'download-branch'

      - name: Checkout Destination Branch in Output Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.MERGE_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.MERGE_BRANCH }}  # set the destination branch to checkout
          token: ${{ secrets.PAT_FG }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'merge-branch'

      - name: Download Artifact with Remote Timestamps Metadata
        uses: actions/download-artifact@v4
        with:
          name: final-playlist-metadata
          path: ${{ github.workspace }}

      - name: Process, Deduplicate, Format and Merge Playlists
        shell: bash
        run: |
          set +e # ensures we see all debug echos even on minor command failures
          TOTAL_GROUP_FAILURES=0 # tracks global failures across the clean and merge processes of all merged playlists, starts from 0
          PLAYLIST_METADATA="${{ github.workspace }}/remote_timestamps.meta"
          touch ./merged_files_to_commit.txt # creates a registry file to track successfully created merged playlists for the commit step

          # Step 1: Use helper function for group-wise cleaning and merging
          process_group() { # function encapsulating cleaning and merging processes for any given merged playlist folder
            local dir="$1" # parent folder (e.g., sl-le)
            local output="$2" # target merged file (e.g., sl-le-all.m3u)
            shift 2
            local sources=("$@") # array of input playlists "m3u/filename.m3u"
            
            echo "-----------------------------------------------------------------------------"
            echo "PROCESS START: Creation of merged playlist $output in folder $dir"
            echo "-----------------------------------------------------------------------------"

            # Sub-step 1a: Clean input playlists for a specific merged playlist
            echo "Starting cleaning of input playlists for merged playlist $output..."
            local successful_san_count=0 # count of successfully cleaned input playlists for this merged playlist, starts from 0
            local cleaned_details="" # record of file-size and count of #EXTINF lines in successfully cleaned input playlists, starts blank

            for src in "${sources[@]}"; do
              local src_path="download-branch/$dir/$src" # constructs absolute path for input playlists e.g., download-branch/sl-le/m3u/sl-le-1.m3u
              local filename=$(basename "$src") # e.g., filename sl-le-1.m3u extracted from src m3u/sl-le-1.m3u 

              # module: skip the input playlist if missing in download repo
              if [ ! -f "$src_path" ]; then # if input playlist is missing in download repo, skip it
                echo "SKIPPED: Playlist $filename not found in repository. Skipping file."
                continue
              fi 

              # module: for input playlist downloaded in Jobs 1 or 2, fetch remote commit timestamp and time-note from the consolidated metadata file
              local meta_entry=$(grep "|$filename|" "$PLAYLIST_METADATA" | head -n 1 || echo "")
              local ts=""
              local note=""
              if [[ -n "$meta_entry" ]]; then
                ts=$(echo "$meta_entry" | cut -d'|' -f5) # extracts the timestamp
                note=$(echo "$meta_entry" | cut -d'|' -f6) # extracts the time-note
                local LAST_MOD="$ts $note" # combines remote timestamp and time note
                local clean_d=$(echo "$ts" | awk '{print $1}' | awk -F'-' '{print $3"-"$2"-"$1}') # remote date in touch-friendly format (YYYY-MM-DD HH:MM)
                local clean_t=$(echo "$ts" | awk '{print $2}') # remote time in touch-friendly format
 
              # module: for pre-existing input playlists in download repo, fallback to file's system time
              else
                local LAST_MOD=$(TZ='Asia/Kolkata' date -r "$src_path" "+%d-%m-%Y %H:%M IST")" ([x]m3u commit time)"
              fi

              # module: clean input playlist (remove carriage returns and BOM) and echo its success/failure metrics
              if sed -i '1s/^\xef\xbb\xbf//; s/\r//g' "$src_path" 2>/dev/null; then # strips Windows line endings (\r) and any non-standard character markers
         
                # sub-module: post-cleaning, apply the extracted remote commit timestamp onto the cleaned input playlist   
                [ -n "$ts" ] && touch -d "$clean_d $clean_t" "$src_path" 2>/dev/null
                
                # sub-module: echo input playlist level success metrics
                successful_san_count=$((successful_san_count + 1))
                local file_size=$(stat -c%s "$src_path") # file size in bytes of the cleaned input playlist
                local extinf_count=$(grep -c "#EXTINF" "$src_path" || true) # count of #EXTINF lines present in the cleaned input playlist
                cleaned_details+="\n  - $filename: last updated $LAST_MOD, $file_size bytes, $extinf_count #EXTINF lines"  # for appending to the summary list for the group-level debug echo
                echo "Cleaned input playlist $filename (size: $file_size bytes). $extinf_count #EXTINF lines found."
              else
                echo "ERROR: Cleaning FAILED for input playlist $filename. Skipping file."
              fi
            done

            # Sub-step 1b: Echo group-level summary report after cleaning input files for this merged playlist
            if [ $successful_san_count -gt 0 ]; then
              echo -e "RESULT: Cleaning of input playlists for merged playlist $output COMPLETED. $successful_san_count input playlists successfully cleaned:$cleaned_details"
            fi

            # Sub-step 1c: Gate - if no input playlists for this merged playlist were found or cleaned, skip its merge process
            if [ $successful_san_count -eq 0 ]; then # merge process for this merged playlist is skipped, workflow continues to cleaning imput playlists for the next merged playlist (ensured by 'return' rather than 'exit')
              echo "RESULT: Merge process for $output FAILED at cleaning stage. Found no valid input playlists or cleaning failed for all input playlists."
              return 1 # exits the process_group function with an error code; TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) logic used in calling the function ensures that the failure is recorded
            fi
 
            # Sub-step 1d: Deduplicate, format and append stream URLs from each cleaned input playlist to this merged playlist
            echo "Starting deduplication and merge of cleaned input playlists into $output..."
            (
              set -e # ensures that subshell exits on critical command failure
              local target_dir="merge-branch/$dir"
              mkdir -p "$target_dir"
              local target="$target_dir/$output" # e.g., merge-branch/sl-le/sl-le-all.m3u
              local TRACKER="/tmp/seen_${output}_$(date +%s).tmp" # creates a unique temp tracker for this group's deduplication

              # module: initialize merged playlist
              echo "#EXTM3U" > "$target" # initializes the merged playlist with standard EXTM3U header
              printf "\n" >> "$target" # appends 1-line trailing spacing
              touch "$TRACKER" # initialize a fresh instance of the tracker at this point

              # module: append streams from a cleaned input playlist to the merged playlist after deduplication, custom spacing and formatting; echo result
              local block_index=0 # block_index tracks which source m3u-block we are currently writing to
              for src in "${sources[@]}"; do
                local src_path="download-branch/$dir/$src"
                local filename=$(basename "$src")
                [ ! -f "$src_path" ] && continue

                # sub-module: re-fetch timestamps for the block header (identical logic to cleaning stage)
                local b_time=$(TZ='Asia/Kolkata' date -r "$src_path" "+%d-%m-%Y %H:%M IST")
                local b_note=$(grep "|$filename|" "$PLAYLIST_METADATA" | head -n 1 | cut -d'|' -f6 || echo "")
                [ -z "$b_note" ] && b_note="([x]m3u commit time)"
                local BLOCK_MOD="$b_time $b_note" 

                # sub-module: awk function for deduplication and custom formatting
                awk -v src_name="$filename" -v tracker="$TRACKER" -v idx="$block_index" -v mod_date="$BLOCK_MOD" -v s=$RANDOM '

                  # pre-processing
                  BEGIN {
                    count=0; tag_buf=""; file_buf=""; found_stream=0; # ignore everything till found_stream turns 1, then start processing everything
                    if ((getline < tracker) > 0) { # pre-load disk tracker - load existing unique stream URLs into a memory array
                      do { seen[$0] = 1 } while ((getline < tracker) > 0)
                      close(tracker)
                    }
                    srand(s) # random mumber generator (for use in randomizing dummy ip address in printf later)
                  } 
    
                  # deduplication
                  /^#EXTINF/ { found_stream=1 } # found_stream: 1 when the first #EXTINF line in the input playlist is found
                  {
                    if (found_stream == 0) next; # discard all lines before first #EXTINF is found, process everything thereon
                    if (/^#/) {
                        tag_buf = tag_buf $0 "\n" # buffer metadata lines
                    } else if (/^(http|https):\/\//) {
                        url = $0 # deduplicate URLs using the group tracker                       
                        if (!seen[url]) { # deduplication: internal memory check
                            print url >> tracker # update disk tracker
                            seen[url] = 1 # update memory array                   
                            if (tag_buf != "") { 
                                file_buf = file_buf tag_buf url "\n" # only prepends the #EXTINF metadata if it's the first URL for this #EXTINF
                                tag_buf = "" # resets the metadata buffer so that the 2nd URL (if available) for the same EXTINF doesnt print the #EXTINF metadata again
                            } else {
                                file_buf = file_buf url "\n" # 2nd URL (if available) for the ame #EXTINF
                            }
                            count++
                        }
                    }
                  }

                  # custom spacing and formatting
                  END {
                    if (count > 0) { # custom spacing logic: block_index 0 = first block (no preceding space), block_index > 0 = subsequent blocks (2-line preceding space)
                      prefix = (idx == 0) ? "" : "\n\n" # if not the first block, prepends 2 empty lines
                      printf "%s# Playlist from %s : %d stream(s) \n", prefix, src_name, count # adds source identifier header
                      printf "# [last updated %s] \n\n", mod_date # adds remote update timestamp for the source playlist + 1-line trail spacing + content
                      printf "#EXTINF:-1,----●★ %s %s ★●----\n", src_name, mod_date # adds block divider
                      printf "http://%d.%d.%d.%d/%08d\n%s", rand()*256, rand()*256, rand()*256, rand()*256, rand()*10^8, file_buf # adds random dummy url for block divider
                    }
                  }                 
                ' "$src_path" >> "$target" # appends input playlists to the merged playlist using the awk logic

                if grep -q "Playlist from $filename" "$target"; then # only increment block_index if this block was successfully added to the target
                   block_index=$((block_index + 1))
                fi        
                echo "Stream URLs from $filename deduplicated and appended. $output now has $(wc -l < "$TRACKER") unique stream URLs."
              done # end of module for deduplication, custom spacing, formatting and appending streams
              
              # module: clean up tracker
              rm -f "$TRACKER"
            )

            # Sub-step 1e: Echo result at end of merge process for this merged playlist
            if [ $? -eq 0 ]; then
              local final_path="merge-branch/$dir/$output"
              local final_size=$(stat -c%s "$final_path")
              local final_extinf=$(grep -c "#EXTINF" "$final_path" || true)
              echo "RESULT: Merge process for $output COMPLETED. $output ($final_size bytes) is ready to push. $final_extinf #EXTINF lines present."  
              return 0
            else
              echo "RESULT: Merge process for $output FAILED at deduplication, fomratting or appending stage." 
              return 1
            fi
          }

          # Step 2: Dynamic discovery and execution loop (using indirect expansion) for playlist groups
          DIRS=$(cut -d'|' -f1 "$PLAYLIST_METADATA" | sed 's|\(.*\)/.*|\1|' | sort -u) # extracts all unique directories in the consolidated metadata file
          for DIR in $DIRS; do # maps the identified directories to merged playlist names
              VAR_SUFFIX=$(echo "$DIR" | tr '-' '_') # creates suffix for the lookup variable from directory name (e.g., sl-le -> sl_le)
              LOOKUP_VAR="MERGED_PLAYLIST_$VAR_SUFFIX" # constructs the lookup variable (e.g., sl-le -> MERGED_PLAYLIST_sl_le)
              OUTPUT_NAME="${!LOOKUP_VAR}" # indirect expansion to get the variable value (e.g., "sl-le-all.m3u")
              if [ -z "$OUTPUT_NAME" ]; then # skips the folder (e.g., sl-le) if its merged playlist variable (e.g., MERGED_PLAYLIST_sl_le) hasn't been defined in the 'env:'
                echo "DEBUG: Skipping group $DIR - no 'env.$LOOKUP_VAR' defined."
                continue
              fi

              # Sub-step 2a: Identify active folder groups (touched by Job 1 and so present in metadata file), then pick all input playlists present in the m3u sub-folder (new + pre-existing)
              if [ -d "download-branch/$DIR/m3u" ]; then # checks if the identified directory (from the metadata file) in the download-branch has a 'm3u' sub-folder 
                # %f returns just the name; we manually prepend 'm3u/' so process_group knows where to look
                mapfile -t FILES_FOR_GROUP < <(find "download-branch/$DIR/m3u" -maxdepth 1 -name "*.m3u" ! -name "$OUTPUT_NAME" -printf "m3u/%f\n" | sort) # for merge process, picks only those folders in download-branch that are present in timestamps metadata file; checks ONLY the 'm3u' subfolder within the parent group and assigns the input playlists within the sub-folder to $OUTPUT_NAME
              fi           

              # Sub-step 2b: Call the process_group function for found input playlists
              if [ ${#FILES_FOR_GROUP[@]} -gt 0 ]; then # checks if any input playlists for this group were actually found
                  echo "-----------------------------------------------------------------------------"
                  echo "DISCOVERY: Found ${#FILES_FOR_GROUP[@]} input M3U playlists in $DIR/m3u subfolder."
                  if process_group "$DIR" "$OUTPUT_NAME" "${FILES_FOR_GROUP[@]}"; then # calls the 'process_group' function with the dynamic folder, merged playlist name, and input playlist array
                      echo "$DIR/$OUTPUT_NAME" >> ./merged_files_to_commit.txt # registers the successfully merged playlist for the commit step
                  else
                      TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) # increments the global error counter everytime the merge function fails 
                  fi
              else
                  echo "DEBUG: No input M3U playlists found in $DIR/m3u subfolder."
              fi
          done             
          
          # Step 3: Export failure count to GitHub environment for final job status
          echo "TOTAL_FAILURES=$TOTAL_GROUP_FAILURES" >> $GITHUB_ENV 

      - name: Commit Changes and Push Merged Playlists to Destination Branch
        run: |
          cd merge-branch

          # Step 1: Configure git             
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG }}@github.com/${{ env.MERGE_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
  
          # Step 2: Load the merged playlists registered in the previous step
          if [ -f "../merged_files_to_commit.txt" ]; then
            mapfile -t FILES < ../merged_files_to_commit.txt
          else
            FILES=()
          fi

          # Step 3: Iterate through the merged playlists to analyse using tracking variables for the final summary echos        
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""         
          for FILE in "${FILES[@]}"; do
            if [ -f "$FILE" ]; then          
              if [[ -n $(git status --porcelain "$FILE") ]]; then # uses git status to determine if the file state is changed or new (and therefore will be committed)
                git add "$FILE" # stage only the changed/new files selectively based on the git status check    
                COMMITTED_NAMES+="$(basename "$FILE") " # adds to committed list and gathers metrics
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SIZE=$(stat -c%s "$FILE") 
                EXTINF=$(grep -c '#EXTINF' "$FILE" || true)
                SUMMARY_DETAILS+="\n  - $(basename "$FILE") ($SIZE bytes): now has $EXTINF #EXTINF lines"
              else # if git status is empty, it means the file is clean or unchanged/identical to the one in repo, and will therefore not be committed
                SKIPPED_NAMES+="$(basename "$FILE") " # adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for merged playlist $(basename "$FILE")."                
              fi
            else
              echo "WARNING: Merged playlist $(basename "$FILE") not found on disk at $(pwd)/$FILE."              
            fi
          done

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit for any merged playlists. Check if files exist at: $(ls -R)"
          else
            git commit -m "Auto-updated merged playlists" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.MERGE_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.MERGE_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Commits were made to $COMMITTED_COUNT merged playlist(s): ${COMMITTED_NAMES:-(none)}"
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT merged playlist(s) were found unchanged and were skipped: ${SKIPPED_NAMES:-(none)}"
          echo "------------------------------------------------------------------------------------------------------------------------"
          
          # Step 5: Final job failure reporting (if group-level errors occurred)
          if [ "${{ env.TOTAL_FAILURES }}" -gt 0 ]; then
            echo "DEBUG: Final Status: FAIL (workflow ended with $TOTAL_FAILURES group failures)"
            exit 1
          fi



  echo_input_message: # echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
    name: Echo Input Message
    runs-on: ubuntu-latest
    steps:
      - name: Print Trigger Message
        run: |
          echo "This workflow run was triggered by: ${{ github.event.inputs.message }}"
