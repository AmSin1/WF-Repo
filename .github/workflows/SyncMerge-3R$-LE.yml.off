name: SyncMerge 3R $ LE

on: 
  workflow_dispatch: # allows manual workflow run from the Actions tab (workflow must be in default branch to work)
    inputs: # optional input(s) that the cronjob service can pass on to the workflow
      message:
        description: 'source trigger of the workflow run'
        required: true
        default: 'manual GitHub WebUI trigger' # default message to be printed if workflow is triggered manually via WebUI or if there is no input message alongside a trigger by cronjob service
        type: string # optionally, specifies type of message
  # internal scehduled run parameters in the 2 lines below 'commented out' as external cronjob scheduler service is engaged
  # schedule: # scheduled to run at designated times or every x minutes (minimum interval allowed is 5 minutes)
    # - cron: '*/22 * * * *'  
      
env:
  CRONJOB_SERVICE: "FastCron"
  DESTINATION_DIR1_PATH: "sl-le"
  DESTINATION_DIR2_PATH: "fc-le"

concurrency: # ensures that only one GitHub workflow runs at a time for a specific branch in a repo, prevents conditions where two separate workflow runs attempt to push to same branch simultaneously
  group: sync-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: false # 'false' ensures that a newly-started workflow runs sequentially after and does not terminate a still in-progress workflow run, ensuring that every workflow's changes are eventually pushed

jobs:   
  download_playlists: # copies specified playlists from source repos and overwrites corresponding specified playlists in destination branch/directories in destination repo
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:
      - name: Validate and Load JSON Config from Repo Secrets
        shell: bash
        env:
          CONFIG_JSON: ${{ secrets.ENV_VARIABLES_LE }}
        run: |
          # Step 1: Validate JSON syntax
          if ! echo "$CONFIG_JSON" | jq empty; then
            echo "ERROR: INVALID JSON detected in repo secret! Check syntax for missing brackets or trailing commas."
            exit 1
          fi          
          
          # Step 2: Export JSON keys/values to GitHub environment
          echo "$CONFIG_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' >> $GITHUB_ENV # extracts all keys and values and writes them to the global $GITHUB_ENV
          
          # Step 3: Mask individual variables for public log security
          echo "$CONFIG_JSON" | jq -r '.[]' | while read -r value; do
            [ -n "$value" ] && echo "::add-mask::$value" # ensures that even after parsing, individual parsed variables are masked as '***' in logs
          done
          echo "SUCCESS: JSON configuration validated and loaded into GitHub environment."
      
      - name: Checkout Destination Branch in Download Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DOWNLOAD_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DOWNLOAD_BRANCH }}  # set the destination branch to checkout
          token: ${{ secrets.PAT_FG_SYNC }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'download-branch'

      - name: Download Playlists from Source Repos  
        run: |
          # Step 1: Define the playlist array
          LIST=$(cat <<EOF # defines the list of playlists to process in a pipe-separated array (URL|DIR|FILE)
          ${{ env.SOURCE_PLAYLIST1a_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-1.m3u
          ${{ env.SOURCE_PLAYLIST1b_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-2.m3u
          ${{ env.SOURCE_PLAYLIST1c_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-3.m3u
          ${{ env.SOURCE_PLAYLIST2a_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-1.m3u
          ${{ env.SOURCE_PLAYLIST2b_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-2.m3u
          ${{ env.SOURCE_PLAYLIST2c_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-3.m3u
          ${{ env.SOURCE_PLAYLIST2d_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-4.m3u
          EOF
          )

          # Step 2: Initialize the metadata file 
          METADATA_FILE="${{ github.workspace }}/remote_timestamps.meta" # initializes the metadata file file to record source playlists' latest remote update timestamps
          touch "$METADATA_FILE" # ensures the initialized file exists

          # Step 3: Download playlists and sync remote timestamps
          echo "$LIST" | while IFS="|" read -r URL DIR FILE; do # extracts data from the playlist array string using IFS pipe separator
            [ -z "$URL" ] && continue
            TARGET="download-branch/$DIR/$FILE" # sets destination file path for each playlist download
            mkdir -p "download-branch/$DIR" # creates destination directory if it doesn't already exist

            # Sub-step 3a: Download each playlist from source repos
            CURL_FLAGS="--connect-timeout 3 --retry 3 --retry-delay 1 --retry-max-time 10 --retry-all-errors --no-progress-meter" # optional curl flags
            HTTP_CODE=$(curl $CURL_FLAGS -R -L --fail \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              -H "User-Agent: FileSync-Batch-2026" \
              -o "$TARGET" -w "%{http_code}" "$URL" || echo "000") # downloads the playlist using curl and captures the HTTP status code to verify download success

            # Sub-step 3b: Fetch and sync remote commit timestamp, and echo success/failure with key metrics for each download
            if [ "$HTTP_CODE" = "200" ] && [ -f "$TARGET" ]; then # checks if playlist exists AND was actually downloaded during this specific workflow run
         
              # module: fetch remote commit timestamps
              REMOTE_DATE="" # starts afresh for this iteration
              TIME_NOTE="(standard download)" # default value
              if [[ "$URL" == *"github.com"* ]] || [[ "$URL" == *"githubusercontent.com"* ]]; then
            
                # sub-module: for downloads from GitHub URLs, capture remote commit timestamps from GitHub API 
                OWNER_REPO=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/([^/]+/[^/]+).*|\2|') # extracts 'owner/repo' to call the GitHub API
                FILE_PATH=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/[^/]+/[^/]+/(blob/\|raw/)?([^/]+/)?(.*)|\4|') # extracts 'path/to/file' to call the GitHub API
                API_URL="https://api.github.com/repos/${OWNER_REPO}/commits?path=${FILE_PATH}&per_page=1" # sets the URL to target the playlist's commit history
                REMOTE_DATE=$(curl -s \
                  -H "Authorization: Bearer ${{ secrets.PAT_FG_SYNC }}" \
                  -H "X-GitHub-Api-Version: 2022-11-28" \
                  -H "User-Agent: GitHub-Actions-FileSync" \
                  "$API_URL" | jq -r '.[0].commit.committer.date' 2>/dev/null || echo "") # captures the remote commit timestamp by querying GitHub API
                [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ] && TIME_NOTE="([✓]API verified)" # updates the timestamp tag
              else
                # sub-module: for downloads from non-GitHub URLs, fallback to capturing remote commit timestamps from HTTP header   
                HEADER_VAL=$(curl -sI "$URL" | grep -i 'Last-Modified:' | cut -d':' -f2- | xargs) # captures the remote commit timestamp from the remote file's header metadata
                if [ -n "$HEADER_VAL" ]; then
                  REMOTE_DATE=$(date -d "$HEADER_VAL" -u +"%Y-%m-%dT%H:%M:%SZ" 2>/dev/null)
                  TIME_NOTE="([✓]header verified)" # updates the timestamp tag
                fi
              fi
              
              # module: sync remote commit timestamps
              if [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ]; then
                touch -d "$REMOTE_DATE" "$TARGET" # manually syncs the downloaded file's timestamp to match remote commit date
                MOD_TIME=$(TZ="Asia/Kolkata" date -d "$REMOTE_DATE" "+%d-%m-%Y %H:%M IST") # formats and converts timestamp to IST
              else
                MOD_TIME=$(TZ="Asia/Kolkata" date -r "$TARGET" "+%d-%m-%Y %H:%M IST") # fallback to downloaded playlist's local commit (workflow run) timestamp in case fetching of remote timestamp via GitHub API or HTTP header fails
                TIME_NOTE="([x]download time)" # updates the timestamp tag
              fi
              
              # module: echo success/failure of each downloaded playlist with key metrics
              SIZE=$(stat -c%s "$TARGET") # calculates file size of downloaded playlist
              COUNT=$(grep -c '#EXTINF' "$TARGET" || true) # counts number of #EXTINF lines in downloaded playlist
              echo "$FILE download : SUCCESS. Last updated $MOD_TIME $TIME_NOTE. $SIZE bytes, $COUNT #EXTINF lines."
              echo "$DIR|$FILE|$SIZE|$COUNT|$MOD_TIME|$TIME_NOTE" >> remote_timestamps.meta # record each playlist's result to metadata fragment (format: DIR|FILE|SIZE|COUNT|TIMESTAMP|TIMENOTE)   
            else
              echo "$FILE download : FAILED (HTTP: $HTTP_CODE)"
              echo "$DIR|$FILE|FAILED|0|N/A|N/A" >> remote_timestamps.meta
            fi
            unset MOD_TIME # clears for next iteration
          done

          # Step 4: Clean up temporary work files       
          rm -f *.tmp *.temp temp_* # 

      - name: Echo Group-Level Download Results
        run: |
          # Step 1: Echo group-level download results
          DIRS=$(cut -d'|' -f1 remote_timestamps.meta | sort -u) # extract unique directories from the consolidated metadata file
          echo "------------------------------------------------------------------------------------------------------------------------"
          for DIR in $DIRS; do
            S_COUNT=0; T_EXTINF=0; T_SIZE=0; F_COUNT=0; F_NAMES=""
            
            # Sub-step 1a: Aggregate group-level download stats for this specific folder
            while IFS="|" read -r f_dir f_name f_size f_extinf f_time f_note; do
              if [ "$f_size" = "FAILED" ]; then
                F_COUNT=$((F_COUNT + 1)) # count of playlists for which download failed 
                F_NAMES+="$f_name " # names of playlists for which download failed 
              else
                S_COUNT=$((S_COUNT + 1)) # count of playlists for which download succeeded
                T_EXTINF=$((T_EXTINF + f_extinf)) # count of #EXTINF lines in playlists for which download succeeded
                T_SIZE=$((T_SIZE + f_size)) # total size of playlists for which download succeeded
              fi
            done < <(grep "^$DIR|" remote_timestamps.meta)
            
            # Sub-step 1b: Echo group-level download results with key metrics
            echo "RESULT for folder $DIR : successfully downloaded $S_COUNT playlists ($T_SIZE bytes). Total $T_EXTINF #EXTINF lines present."
            echo "RESULT for folder $DIR : failed to download $F_COUNT playlists: ${F_NAMES:-(none)}"
            echo "------------------------------------------------------------------------------------------------------------------------"
          done

      - name: Commit Changes and Push to Destination Branch
        run: |
          # Step 1: Enter destination directory
          cd download-branch # changes current working directory to the destination branch    

          # Step 2: Configure git          
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG_SYNC }}@github.com/${{ env.DOWNLOAD_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
          
          # Step 3: Analyse downloaded playlists for final summary echos, and stage changed/new files for commit 
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""
          while IFS="|" read -r DIR FILE SIZE COUNT TIMESTAMP NOTE; do
            TARGET="$DIR/$FILE"
            if [ -f "$TARGET" ]; then
              if [[ -n $(git status --porcelain "$TARGET") ]]; then # uses git status to determine the file state
                COMMITTED_NAMES+="$FILE " # playlist is changed or new and will be committed; adds to committed list
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SUMMARY_DETAILS+="\n  - $FILE : last updated $TIMESTAMP $NOTE - $SIZE bytes, $COUNT #EXTINF lines"
                git add "$TARGET" # stages only the changed/new files selectively based on the git status check 
              else
                SKIPPED_NAMES+="$FILE " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for playlist $FILE."
              fi
            fi
          done < ../remote_timestamps.meta

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit in any of the $SKIPPED_COUNT playlists. Skipping push."
          else
            git commit -m "Automated sync from source" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DOWNLOAD_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DOWNLOAD_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Successfully committed and pushed $COMMITTED_COUNT updated playlist(s)."
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT playlist(s) were found unchanged and were skipped: ${SKIPPED_NAMES:-(none)}"
          echo "------------------------------------------------------------------------------------------------------------------------"

      - name: Upload Artifact with Remote Timestamps Metadata
        if: always() # ensures metadata is uploaded even if some downloads in the batch fail
        uses: actions/upload-artifact@v4
        with:
          name: playlist-metadata
          path: ${{ github.workspace }}/remote_timestamps.meta # final source-of-truth for all remote commit timestamps
          if-no-files-found: error # forces this job to fail if the playlist-metadata file is missing


  
  create_merged_playlists: # creates custom-formatted deduplicated merged playlists with prioritized m3u comtent-blocks from input playlists
    needs: download_playlists # forces this job to wait for the referenced job to finish (forces sequential instead of the default parallel execution)
    if: always() # this job runs regardless of other jobs' success/failure status
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:          
      - name: Validate and Load JSON Config from Repo Secrets
        shell: bash
        env:
          CONFIG_JSON: ${{ secrets.ENV_VARIABLES_LE }}
        run: |
          # Step 1: Validate JSON syntax
          if ! echo "$CONFIG_JSON" | jq empty; then
            echo "ERROR: INVALID JSON detected in repo secret! Check syntax for missing brackets or trailing commas."
            exit 1
          fi          
          
          # Step 2: Export JSON keys/values to GitHub environment
          echo "$CONFIG_JSON" | jq -r 'to_entries[] | "\(.key)=\(.value)"' >> $GITHUB_ENV # extracts all keys and values and writes them to the global $GITHUB_ENV
          
          # Step 3: Mask individual variables for public log security
          echo "$CONFIG_JSON" | jq -r '.[]' | while read -r value; do
            [ -n "$value" ] && echo "::add-mask::$value" # ensures that even after parsing, individual parsed variables are masked as '***' in logs
          done
          echo "SUCCESS: JSON configuration validated and loaded into GitHub environment."

      - name: Checkout Source Branch in Download Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DOWNLOAD_REPO }}' # specify 'owner/repo' of the source repo
          ref: ${{ env.DOWNLOAD_BRANCH }}  # set the source branch to checkout
          token: ${{ secrets.PAT_FG_SYNC }}
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'download-branch'

      - name: Checkout Destination Branch in Merge Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.MERGE_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.MERGE_BRANCH }}  # set the destination branch to checkout
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'merge-branch'

      - name: Download Artifact with Remote Timestamps Metadata
        uses: actions/download-artifact@v4
        with:
          name: playlist-metadata 
          path: ${{ github.workspace }}

      - name: Process, Deduplicate, Format and Merge Playlists
        run: |
          TOTAL_GROUP_FAILURES=0 # tracks global failures across the clean and merge processes of all merged playlists, starts from 0
          PLAYLIST_METADATA="${{ github.workspace }}/remote_timestamps.meta"
          touch ../merged_files_to_commit.txt # creates a registry file to track successfully created merged playlists for the commit step

          # Step 1: Use helper function for group-wise cleaning and merging
          process_group() { # function encapsulating cleaning and merging processes for any given merged playlist folder
            local dir="$1"
            local output="$2"
            shift 2
            local sources=("$@")
            
            echo "-----------------------------------------------------------------------------"
            echo "PROCESS START: Creation of merged playlist $output in folder $dir"
            echo "-----------------------------------------------------------------------------"

            # Sub-step 1a: Clean input playlists for a specific merged playlist
            echo "Starting cleaning of input playlists for merged playlist $output..."
            local successful_san_count=0 # count of successfully cleaned input playlists for this merged playlist, starts from 0
            local cleaned_details="" # record of file-size and count of #EXTINF lines in successfully cleaned input playlists, starts blank

            for src in "${sources[@]}"; do
              local src_path="download-branch/$dir/$src"

              # module: fallback to playlist in repository if current download of an input playlist fails 
              if [ ! -f "$src_path" ]; then # if input playlist is missing (current download failed), uses the existing playlist in the repo for merge process
                if [ -f "merge-branch/$dir/$src" ]; then
                  echo "FALLBACK: Current download not found for playlist $src. Using existing previous instance from repository."
                  mkdir -p "download-branch/$dir"
                  cp "merge-branch/$dir/$src" "$src_path" # pulls the existing playlist from merge repo into processing directory
                else
                  echo "SKIPPED: Download failed for playlist $src AND no previous instance found in repository. Skipping file."
                  continue
                fi
              fi

              # module: fetch remote timestamp and time-note for the input playlist from the consolidated metadata file
              local meta_entry=$(grep "|$src|" "$PLAYLIST_METADATA" || echo "")
              local LAST_MOD="" # initializes file to capture remote timestamps and time-notes from the consolidated metadata file
              if [[ -n "$meta_entry" && "$(echo "$meta_entry" | cut -d'|' -f3)" != "FAILED" ]]; then   
                # sub-module: for input playlist successfully downloaded in this workflow run, extract from condolidated metadata file
                local ts=$(echo "$meta_entry" | cut -d'|' -f5) # extracts field 5 (remote timestamp) 
                local note=$(echo "$meta_entry" | cut -d'|' -f6) # extracts field 6 (time note)
                LAST_MOD="$ts $note" # combines remote timestamp and time note
              else
                # sub-module: for input playlist not downloaded in this worflow run, fallback to timestamp of the existing file in the repo
                local ts=$(TZ='Asia/Kolkata' date -r "$src_path" "+%d-%m-%Y %H:%M IST")
                LAST_MOD="$ts ([x]repo fallback)"
              fi

              # module: clean input playlist (remove carriage returns and BOM) and echo its success/failure metrics
              if (set -e; sed -i '1s/^\xef\xbb\xbf//; s/\r//g' "$src_path"); then # strips Windows line endings (\r) and any non-standard character markers
                successful_san_count=$((successful_san_count + 1))
                local file_size=$(stat -c%s "$src_path") # file size in bytes of the cleaned input playlist
                local extinf_count=$(grep -c "#EXTINF" "$src_path" || true) # count of #EXTINF lines present in the cleaned input playlist
                cleaned_details+="\n  - $src: last updated $LAST_MOD, $file_size bytes, $extinf_count #EXTINF lines"  # for appending to the summary list for the group-level debug echo
                echo "Cleaned input playlist $src (size: $file_size bytes). $extinf_count #EXTINF lines found." # debug for cleaning step for individual input playlists
              else
                echo "ERROR: Cleaning FAILED for input playlist $src or valid playlist $src NOT FOUND. Skipping file."
              fi
            done

            # Sub-step 1b: Echo group-level summary report after cleaning input files for this merged playlist
            if [ $successful_san_count -gt 0 ]; then
              echo -e "RESULT: Cleaning of input playlists for merged playlist $output COMPLETED. $successful_san_count input playlists successfully cleaned:$cleaned_details"
            fi

            # Sub-step 1c: Gate - if no input playlists for this merged playlist were cleaned, skip its merge process
            if [ $successful_san_count -eq 0 ]; then # gate: if none of the input playlists for this merged playlist were successfully cleaned, skips its merge process, workflow continues to cleaning imput playlists for the next merged playlist (ensured by 'return' rather than 'exit')
              echo "RESULT: Merge process for $output FAILED at cleaning stage. Found no valid input playlists or cleaning failed for all input playlists."
              return 1 # exits the process_group function with an error code; TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) logic used in calling the function ensures that the failure is recorded
            fi
 
            # Sub-step 1d: Deduplicate, format and append stream URLs from each cleaned input playlist to this merged playlist
            echo "Starting deduplication and merge of cleaned input playlists into $output..."
            (
              set -e # ensures that subshell exits on critical command failure
              local target_dir="merge-branch/$dir"
              mkdir -p "$target_dir"
              local target="$target_dir/$output"
              local TRACKER="/tmp/seen_${output}_$(date +%s).tmp" # creates a unique temp tracker for this group's deduplication

              # module: initialize merged playlist
              echo "#EXTM3U" > "$target" # initializes the merged playlist with standard EXTM3U header
              printf "\n" >> "$target" # adds 1-line trailing spacing
              touch "$TRACKER" # initialize a fresh instance of the tracker at this point

              # module: append streams from a cleaned input playlist to the merged playlist after deduplication, custom spacing and formatting; echo result
              local block_index=0 # block_index tracks which source m3u-block we are currently writing to
              for src in "${sources[@]}"; do
                local src_path="download-branch/$dir/$src"
                [ ! -f "$src_path" ] && continue

                # sub-module: fetch-timestamps for the block-level header (similar logic to cleaning stage)
                local meta_entry=$(grep "|$src|" "$PLAYLIST_METADATA" || echo "")
                local LAST_MOD=""
                if [[ -n "$meta_entry" && "$(echo "$meta_entry" | cut -d'|' -f3)" != "FAILED" ]]; then
                  LAST_MOD="$(echo "$meta_entry" | cut -d'|' -f5) $(echo "$meta_entry" | cut -d'|' -f6)"
                else
                  LAST_MOD="$(TZ='Asia/Kolkata' date -r "$src_path" '+%d-%m-%Y %H:%M IST') ([x]repo fallback)"
                fi

                # sub-module: awk function for deduplication and custom formatting
                awk -v src_name="$src" -v tracker="$TRACKER" -v idx="$block_index" -v mod_date="$LAST_MOD" '

                  # pre-processing
                  BEGIN {
                    count=0; tag_buf=""; file_buf=""; found_stream=0; # ignore everything till found_stream tuens 1, then start processing everything
                    if ((getline < tracker) > 0) { # pre-load disk tracker - load existing unique stream URLs into a memory array
                      do { seen[$0] = 1 } while ((getline < tracker) > 0)
                      close(tracker)
                    }
                  } 

                  # deduplication
                  /^#EXTINF/ { found_stream=1 } # found_stream: 1 when the first #EXTINF line in the input playlist is found
                  {
                    if (found_stream == 0) next; # discard all lines before first #EXTINF is found, process everything thereon
                    if (/^#/) {
                        tag_buf = tag_buf $0 "\n" # buffer metadata lines
                    } else if (/^(http|https):\/\//) {
                        url = $0 # deduplicate URLs using the group tracker                       
                        if (!seen[url]) { # deduplication: internal memory check
                            print url >> tracker #   update disk tracker
                            seen[url] = 1 # update memory array                      
                            file_buf = file_buf tag_buf url "\n"
                            count++
                        }
                        tag_buf = "" # reset metadata buffer for next stream URL
                    }
                  }

                  # custom spacing and formatting
                  END {
                    if (count > 0) { # custom spacing logic: block_index 0 = first block (no preceding space), block_index > 0 = subsequent blocks (2-line preceding space)
                      prefix = (idx == 0) ? "" : "\n\n" # if not the first block, prepend 2 empty lines
                      printf "%s# Playlist from %s : %d stream(s) \n", prefix, src_name, count # add source identifier header
                      printf "# [last updated %s] \n\n%s", mod_date, file_buf # add remote update timestamp for the source playlist + 1-line trail spacing + content
                    }
                  }                 
                ' "$src_path" >> "$target" # appends input playlists to the merged playlist using the awk logic

                if grep -q "Playlist from $src" "$target"; then # only increment block_index if this block was successfully added to the target
                   block_index=$((block_index + 1))
                fi        
                echo "Stream URLs from $src deduplicated and appended. $output now has $(wc -l < "$TRACKER") unique stream URLs."
              done # end of module for deduplication, custom spacing, formatting and appending streams
              
              # module: clean up tracker
              rm -f "$TRACKER"
            )

            # Sub-step 1e: Echo result at end of merge process for this merged playlist
            if [ $? -eq 0 ]; then
              local final_path="merge-branch/$dir/$output"
              local final_size=$(stat -c%s "$final_path")
              local final_extinf=$(grep -c "#EXTINF" "$final_path" || true)
              echo "RESULT: Merge process for $output COMPLETED. $output ($final_size bytes) is ready to push. $final_extinf #EXTINF lines present." # debug at the end of merge process
              return 0
            else
              echo "RESULT: Merge process for $output FAILED at deduplication, fomratting or appending stage." 
              return 1
            fi
          }

          # Step 2: Dynamic execution loop (using indirect expansion) for playlist groups
          DIRS=$(cut -d'|' -f1 "$PLAYLIST_METADATA" | sort -u) # extracts all unique directories in the consolidated metadata file
          for DIR in $DIRS; do # maps the identified directories to merged playlist names
              VAR_SUFFIX=$(echo "$DIR" | tr '-' '_') # creates suffix for the lookup variable from directory name (e.g., sl-le -> sl_le)
              LOOKUP_VAR="MERGED_PLAYLIST_$VAR_SUFFIX" # constructs the lookup variable (e.g., sl-le -> MERGED_PLAYLIST_sl_le)
              OUTPUT_NAME="${!LOOKUP_VAR}" # indirect expansion to get the variable value (e.g., "sl-le-all.m3u")
              if [ -z "$OUTPUT_NAME" ]; then # skips the folder (e.g., sl-le) if its merged playlist variable (e.g., MERGED_PLAYLIST_sl_le) hasn't been defined in the 'env:'
                echo "DEBUG: Skipping group $DIR - no 'env.$LOOKUP_VAR' defined."
                continue
              fi
              mapfile -t FILES_FOR_GROUP < <(grep "^$DIR|" "$PLAYLIST_METADATA" | cut -d'|' -f2) # extracts only the input playlist names belonging to current group, and puts these filenames into an array
              if [ ${#FILES_FOR_GROUP[@]} -gt 0 ]; then # checks if any input playlists for this group were actually found
                  if process_group "$DIR" "$OUTPUT_NAME" "${FILES_FOR_GROUP[@]}"; then # calls the 'process_group' function with the dynamic folder, merged playlist name, and input playlist array
                      echo "$DIR/$OUTPUT_NAME" >> ./merged_files_to_commit.txt # registers the successfully merged playlist for the commit step
                  else
                      TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) # increments the global error counter everytime the merge function fails 
                  fi
              fi
          done             
          
          # Step 3: Export failure count to GitHub environment for final job status
          echo "TOTAL_FAILURES=$TOTAL_GROUP_FAILURES" >> $GITHUB_ENV 

      - name: Commit Changes and Push Merged Playlists to Destination Branch
        run: |
          cd merge-branch

          # Step 1: Configure git             
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG_SYNC }}@github.com/${{ env.MERGE_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
  
          # Step 2: Dynamically load the merged playlists registered in the previous step
          if [ -f "../merged_files_to_commit.txt" ]; then
            mapfile -t FILES < ../merged_files_to_commit.txt
          else
            FILES=()
          fi

          # Step 3: Iterate through the merged playlists to analyse using tracking variables for the final summary echos        
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""         
          for FILE in "${FILES[@]}"; do
            if [ -f "$FILE" ]; then          
              if [[ -n $(git status --porcelain "$FILE") ]]; then # uses git status to determine if the file state is changed or new (and therefore will be committed)
                git add "$FILE" # stage only the changed/new files selectively based on the git status check    
                COMMITTED_NAMES+="$(basename "$FILE") " # adds to committed list and gathers metrics
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SIZE=$(stat -c%s "$FILE") 
                EXTINF=$(grep -c '#EXTINF' "$FILE" || true)
                SUMMARY_DETAILS+="\n  - $(basename "$FILE") ($SIZE bytes): now has $EXTINF #EXTINF lines"
              else # if git status is empty, it means the file is clean or unchanged/identical to the one in repo, and will therefore not be committed
                SKIPPED_NAMES+="$(basename "$FILE") " # adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for merged playlist $(basename "$FILE")."                
              fi
            else
              echo "WARNING: Merged playlist $(basename "$FILE") not found on disk at $(pwd)/$FILE."              
            fi
          done

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit for any merged playlists. Check if files exist at: $(ls -R)"
          else
            git commit -m "Auto-updated merged playlists" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.MERGE_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.MERGE_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Commits were made to $COMMITTED_COUNT merged playlist(s): ${COMMITTED_NAMES:-(none)}"
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT merged playlist(s) were found unchanged and were skipped: ${SKIPPED_NAMES:-(none)}"
          echo "------------------------------------------------------------------------------------------------------------------------"
          
          # Step 5: Final job failure reporting         
          if [ "${{ env.TOTAL_FAILURES }}" -gt 0 ]; then
            echo "DEBUG: Final Status: FAIL (workflow ended with $TOTAL_FAILURES group failures)"
            exit 1
          fi



  print_trigger_message: # echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
    runs-on: ubuntu-latest
    steps:
      - name: Print Trigger Message
        run: |
          echo "This workflow run was triggered by: ${{ github.event.inputs.message }}"
